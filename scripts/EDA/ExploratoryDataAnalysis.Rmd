---
title: "Exploratory Data Analysis"
author: "Ian Douglas"
date: "4/15/2021"
output: html_document
---

## Setup
```{r}
library(tidyverse)
library(magrittr)
box_path = '~/Box/Research/covid19'
read.csv.box = function(.path)
{
  read.csv(Sys.glob(file.path(box_path, .path)), stringsAsFactors = F)
}
readRDS.box = function(.path)
{
  readRDS(Sys.glob(file.path(box_path, .path)))
}
# Read in the data
dat = readRDS.box("data/T*/Raw*rds")
```


# Correlation of positive and negative effect
```{r}
dat %>%
  ggplot() +
  geom_jitter(aes(exp.NegativeEffect, exp.PositiveEffect)) +
  ggtitle(paste0('Correlation p-value: ', 
                 round(with(dat, cor.test(exp.NegativeEffect, exp.PositiveEffect)$p.val), 3)))
```
No correlation.


# Have any participants had covid?
```{r}
dat %>% 
  filter(exp.EverTested <= 2) %>%
  select(exp.EverPositiveTest) %>% table(., useNA = 'ifany')
```
Seven participants have had COVID.

# Have any participants' friends or family died of COVID?
```{r}
dat %>% filter(exp.AnyoneInHouseDied == 1) %>%
  select(contains('.died.'))
```
6 participants know a family member who died from COVID


# Most commonly chosen negative and positive experiences
```{r}
dat %>%
  select(ID, contains(c('exp.most.neg.', 'exp.most.positive.'))) %>%
  pivot_longer(-ID, names_to = 'Variable', values_to='Value') %>%
  mutate(subscale = str_extract(Variable, 'neg|positive'),
         subscale = case_when(subscale=='neg' ~ 'MostNegative', TRUE ~ 'MostPositive')) %>%
  group_by(Variable, subscale) %>%
  summarise(across(Value, mean)) %>%
  rename(PercentEndorsed = Value) %>%
  group_by(subscale) %>%
  arrange(desc(PercentEndorsed), .by_groups = T) %>%
  mutate(Variable = sub('exp.most.[[:alpha:]]+.', '', Variable),
         Variable = factor(Variable, levels=rev(unique(Variable)))) %>%
  ggplot() + 
  geom_col(aes(x = Variable, y = PercentEndorsed, fill = subscale)) +
  coord_flip()
```


# Most commonly endorsed emotions
```{r}
dat %>%
  select(ID, matches(c('emo.past.7.days.[[:alpha:]]+$'))) %>%
  pivot_longer(-ID, names_to = 'Variable', values_to='Value') %>%
  group_by(Variable) %>%
  mutate(Value = scale(Value)) %>%
  group_by(Variable) %>%
  mutate(means = mean(Value),
         Variable = factor(Variable, levels = rev(unique(Variable)))) %>%
  summarise(Value_Mean = mean(Value, na.rm = T),
            Value_SE = mean(Value, na.rm = T)/length(na.omit(Value))) %>%
  arrange(desc(Value_Mean)) %>%
  mutate(Variable = sub('emo.past.7.days\\.', '', Variable),
         Variable = factor(Variable, levels=rev(unique(Variable)))) %>%
  ggplot() + 
  geom_point(aes(x = Variable, y = Value_Mean)) +
  geom_linerange(aes(x = Variable, ymin = Value_Mean-(1.96*Value_SE), 
                     ymax = Value_Mean+(1.96*Value_SE))) +
  coord_flip()
```


# EFA of behaviors

DIDN'T REALLY WORK, NEED TO ENTER THE CORRECT VARIABLES AND RE-RUN
<!-- ```{r} -->
<!-- library(psych) -->
<!-- beh1 = dat %>% select(contains('cope.behavior')) %>% select(-contains('Other')) -->
<!-- names(beh1) -->
<!-- beh.cor1 = cor(beh1) -->
<!-- beh.para1 = fa.parallel(beh.cor1, nrow(beh1)) -->
<!-- efa1 = fa(r = beh.cor1, n.obs = nrow(beh1), nfactors = beh.para1$nfact, rotate = 'none', fm='ml') -->
<!-- View(GPArotation::Varimax(loadings(efa1))$loadings %>% unclass %>% as.matrix) -->
<!-- ``` -->

# EFA of beahvioral taks variables
```{r}
ef_beh = read.csv.box("data/EF_Behavioral/*summary*csv")
```

## Select the Executive functioning task variables

-For this rudimentary analysis, take the most recent EF score, so that it is the \
closest in time to the COVID surveys
```{r}
ef_beh %>%
  # Select from the first EF column to the last EF column
  select(ID=participant_id, year, PatternCompTotal:NBackSqRt_totFinal) %>%
  # Replace any Inf with NA
  mutate(across(-ID, ~replace(., !is.finite(.), NA))) %>%
  # Taking the most recent timepoint, but add a caveat:
  # Assess the most recent, and one before it.
  # Use coalesce to just replace any NA in the most recent with one before it.
  # If there are still NA in the one before it, then they will stay NA.
  # We don't take any data from any further back so as to keep the EF data mostly recent.
  ungroup %>%
  group_by(ID) %>%
  arrange(desc(year)) %>%
  slice(1:2) %>% # most recent, then the one before it
  select(-year) %>%
  group_by(ID) %>%
  # Hand-coded coalesce
  summarise(across(everything(), ~{
    if (!is.na(.[1])) {.[1]} else if (!is.na(.[2])) .[2] else NA
  })) %>% ungroup -> ef
  

ef.par = fa.parallel(cor(ef[-1], use ='pairwise.complete'), n.obs = sum(complete.cases(ef)))

ef.efa = fa(cor(ef[-1], use ='pairwise.complete'), n.obs = sum(complete.cases(ef)),
            ef.par$nfact, fm='pa', rotate = 'varimax')

# Get the results and correlate with wisc and wais
ef.latentVars = cbind(ef[1], predict(ef.efa, ef[-1]))
ef_wisc_wais = ef_beh %>%
  # Select from the first EF column to the last EF column
  select(ID=participant_id, year, 
         matches(c("^wisc.+total_t$", "^wais.+total_t$"))) %>%
  # Replace any Inf with NA
  mutate(across(-ID, ~replace(., !is.finite(.), NA))) %>%
  ungroup %>%
  group_by(ID) %>%
  arrange(desc(year)) %>%
  slice(1:2) %>% # most recent, then the one before it
  select(-year) %>%
  group_by(ID) %>%
  summarise(across(everything(), ~{
    if (!is.na(.[1])) {.[1]} else if (!is.na(.[2])) .[2] else NA
  })) %>% ungroup

ef_LV_IQ_df = left_join(
  ef.latentVars,
  ef_wisc_wais,
  by = 'ID',
  all.y = F
) %>% mutate(across(matches('^PA[0-9]+$'), scale))

# cor(ef_LV_IQ_df[-1], use = 'pairwise.complete')
ef_scl = ef %>% mutate(across(-ID, scale))
```


# The factors are too correlated, use PCA instead
```{r}
ef.pca = eigen(cor(ef[-1], use = 'pairwise.complete'))
ef.pca_loadings = ef.pca$vectors
rownames(ef.pca_loadings) <- names(ef[-1])
# Rotate the dat
ef.pca.LV = as.matrix(replace(ef_scl[-1],is.na(ef_scl[-1]), 0))%*%ef.pca$vectors[,1:7]
ef.pca.LV.df = cbind(ef_scl[1], ef.pca.LV)
# Merge with IQ vars
ef_pcaLV_IQ_df = left_join(
  ef.pca.LV.df,
  ef_wisc_wais,
  by = 'ID',
  all.y = F
)

# Visualize the cross correlations
cor(ef_pcaLV_IQ_df[-1], use = 'pairwise.complete')[-1:-7, 1:7] %>%
  as.data.frame %>%
  rownames_to_column('IQ_Outcome') %>%
  rename_at(vars(-IQ_Outcome), ~paste0('EF_PC', .)) %>%
  pivot_longer(-IQ_Outcome, names_to='ExecutiveFunctionComponent', values_to = 'Correlation') %>%
  ggplot(aes(x = ExecutiveFunctionComponent, y = IQ_Outcome, fill = Correlation)) +
  geom_tile() +
  scale_fill_viridis_c(option = 'magma')
```


# Fit all bivariate regression models
```{r}
pca.EF.model_df = ef_pcaLV_IQ_df %>% 
  rename_at(vars(matches('[[:digit:]]+')), ~paste0('EF_PC', .))
EF_PC.df = pca.EF.model_df %>% select(ID, starts_with('EF_PC'))
iq.df = pca.EF.model_df %>% select(ID, starts_with('w'))
# fit models:
imap(EF_PC.df[-1], function(y, .y) {
  imap(iq.df[-1], function(x, .x) {
    obj = lm(scale(y) ~ x) %>% summary 
    obj$Title = paste0(.x, ' predicting ', .y)
    obj
  })
}) %>% reduce(c) %>%
  Filter(x = ., f = function(x) x$coef[2,4] < .05) -> all_mods

walk(all_mods, ~{
  print(.x$Title); print(.x)
})
```


# See how these EF components relate to factors from time 1
```{r}
library(GPArotation); library(psych)
time1_fa = readRDS.box("output/FactorAnalysis/no-rotation*list.rds")
time1_fadata = readRDS("~/Box/Church_Lab/EF_Study/COVID_Study/data/factorAnalysisData.rds")
# Get scores
time1_LV = factor.scores(
  x = time1_fadata[-1] %>% as.matrix, 
  f = time1_fa$fa.ml.10$rotated_loadings$Varimax, # varimax-rotated loadings
  method = 'Thurstone'
) %>% pluck('scores')

# Run the regressions as above, but relating EF to behavioral factors
time1_lv.df = cbind(time1_fadata[1], time1_LV)
# Fit models.
# This time predict the COVID factors from (earlier) EF
imap(time1_lv.df[-1], function(y, .y) {
  imap(EF_PC.df[-1], function(x, .x) {
    # Merge them, so we can use the overlapping subjects from the diff. timepoints
    y.df = tibble(ID = time1_lv.df$ID, Y = scale(y))
    x.df = tibble(ID = EF_PC.df$ID, X = x)
    Z = inner_join(x.df, y.df, by = 'ID')
    # Fit model:
    obj = lm(Y ~ X, Z) %>% summary 
    obj$Title = paste0(.x, ' predicting ', .y)
    obj
  })
}) %>% reduce(c) %>%
  Filter(x = ., f = function(x) x$coef[2,4] < .05) -> time1LV.EFPCA.mods

walk(time1LV.EFPCA.mods, ~{
  print(.x$Title); print(.x)
})
```


# See how time 2 emotions map on to the time1 behavior factors
```{r}
time2_emo = dat %>%
  select(ID, matches(c('emo.past.7.days.[[:alpha:]]+$'))) %>%
  rename_at(vars(-ID), ~sub('emo\\.past\\.7\\.days\\.', '', .))
# Fit models as before
imap(time2_emo[-1], function(y, .y) {
  imap(time1_lv.df[-1], function(x, .x) {
    # Merge them, so we can use the overlapping subjects from the diff. timepoints
    y.df = tibble(ID = time2_emo$ID, Y = scale(y))
    x.df = tibble(ID = time1_lv.df$ID, X = scale(x))
    Z = inner_join(x.df, y.df, by = 'ID')
    # Fit models:
    obj = lm(Y ~ X, Z) %>% summary 
    obj$Title = paste0(.x, ' predicting ', .y)
    obj
  })
}) %>% reduce(c) %>%
  Filter(x = ., f = function(x) x$coef[2,4] < .05) -> time2emo.time1LV.mods

# Print results
walk(time2emo.time1LV.mods, ~{
  print(.x$Title); print(.x)
})
```

# Re fit significant  models
```{r}
time2_emo.time1_LV.df_merged = inner_join(
  time2_emo,
  time1_lv.df,
  by = 'ID'
)

summary(lm(Appreciative ~ ML5 + ML7 + ML9, data = time2_emo.time1_LV.df_merged))
```


# Relating EF to emotions at time 2
```{r}
time1_EF.time2_emo.df_merged = inner_join(
  EF_PC.df, time2_emo, by = 'ID'
)
EF.emo.predictors = select(time1_EF.time2_emo.df_merged, ID, starts_with('EF'))
EF.emo.responses = select(time1_EF.time2_emo.df_merged, -starts_with('EF'))
# Fit models
imap(EF.emo.responses[-1], function(y, .y) {
  imap(EF.emo.predictors[-1], function(x, .x) {
    obj = lm(y ~ x) %>% summary 
    obj$Title = paste0(.x, ' predicting ', .y)
    obj
  })
}) %>% reduce(c) %>%
  Filter(x = ., f = function(x) x$coef[2,4] < .05) -> time2emo.time1EFPCA.mods

# Print results
walk(time2emo.time1EFPCA.mods, ~{
  print(.x$Title); print(.x)
})
```

